{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d11d2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomic\n",
    "from nomic import embed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import ollama\n",
    "from ollama import chat\n",
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from yfiles_jupyter_graphs_for_neo4j import Neo4jGraphWidget\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b52d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import deeplake\n",
    "#ds = deeplake.load('hub://activeloop/flickr30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c297a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_SERVER_URL = \"bolt://localhost:7687\"\n",
    "NEO4J_DB_NAME= \"ragdb\"\n",
    "NEO4J_LOGIN = os.environ['NEO4J_USER_LOGIN']\n",
    "NEO4J_PWD = os.environ['NEO4J_USER_PWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7139e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL = 'DC1LEX/nomic-embed-text-v1.5-multimodal'\n",
    "VISION_EMBEDDING_MODEL = 'nomic-embed-vision-v1.5'\n",
    "MULTIMODAL_INFERENCE_MODEL = \"gemma3:4b\"\n",
    "MULTIMODAL_LLAVA_MODEL = \"llava:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48ea2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DATA_FILENAME = \"rag_data.csv\"\n",
    "RAG_DATA_IMG_COL_NAME = \"image_filename\"\n",
    "RAG_DATA_DOC_COL_NAME = \"doc\"\n",
    "RAG_IMG_FOLDER = \"images\"\n",
    "RAG_MAX_GRAPH_DEPTH = 2\n",
    "RAG_QUERY_NUM_TOP_RESULTS = 3\n",
    "USER_QUERY_IMAGE_SEARCH_FOLDER = \"user_image_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_img_path(filename):\n",
    "    local_path = f\"{RAG_IMG_FOLDER}/{filename}\"\n",
    "    if os.path.isfile(local_path):\n",
    "        return local_path\n",
    "    else :\n",
    "        print(f\"File {local_path} does not exist or is not a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5791532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_img_search_path(filename):\n",
    "    local_path = f\"{USER_QUERY_IMAGE_SEARCH_FOLDER}/{filename}\"\n",
    "    if os.path.isfile(local_path):\n",
    "        return local_path\n",
    "    else :\n",
    "        print(f\"File {local_path} does not exist or is not a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293df4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img_path):\n",
    "    encoded_image = None\n",
    "\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        encoded_image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    return encoded_image\n",
    "\n",
    "def encode_images(img_paths):\n",
    "    return [encode_image(img_path) for img_path in img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060a09c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(TEXT_EMBEDDING_MODEL)\n",
    "ollama.pull(MULTIMODAL_INFERENCE_MODEL)\n",
    "ollama.pull(MULTIMODAL_LLAVA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5311c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_embedding(img_path):\n",
    "    nomic_api_key = os.environ['NOMIC_API_KEY'] \n",
    "\n",
    "    nomic.login(nomic_api_key)\n",
    "\n",
    "    output = embed.image(\n",
    "        images=[img_path\n",
    "        ],\n",
    "        model=VISION_EMBEDDING_MODEL,\n",
    "    )\n",
    "\n",
    "    #print(output['usage'])\n",
    "    img_embeddings = np.array(output['embeddings'])\n",
    "    #print(img_embeddings)\n",
    "    #print(img_embeddings.shape)\n",
    "    \n",
    "    return img_embeddings[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77cc7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(txt):\n",
    "    response = ollama.embed(model=TEXT_EMBEDDING_MODEL, input=txt)\n",
    "    txt_embedding = response[\"embeddings\"]\n",
    "    return txt_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5141d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_from_text_prop(graph_doc, txt):\n",
    "    #result = None\n",
    "    for node in graph_doc.nodes:\n",
    "        txt_value = node.properties.get(\"text\")\n",
    "        if txt_value is not None and txt_value == txt:\n",
    "            return node\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21721f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mllm_txt_search(txt_search, ctx):\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Answer the question while taking into account the context. \\n question:{txt_search} \\n context:{ctx}\",\n",
    "                #\"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = response['message']['content']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b483f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_mllm_img_desc(instruction, file_path):\n",
    "    try:\n",
    "        # Ensure the image exists and is accessible\n",
    "        with open(file_path, 'rb') as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "        result = ollama.generate(\n",
    "            model=MULTIMODAL_LLAVA_MODEL,\n",
    "            prompt=instruction,\n",
    "            images=[image_data], # Pass image data directly\n",
    "            stream=False\n",
    "        )\n",
    "        #print(result['response'])\n",
    "        return result['response']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab8737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_text(node):\n",
    "    node_txt = node.get(\"text\")\n",
    "    if node_txt is not None :\n",
    "        return node_txt\n",
    "    else:\n",
    "        return node.get(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04149aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_rag(neo4j_results):\n",
    "        context_list = []\n",
    "        for record in neo4j_results:\n",
    "            # Extract relevant information from the record\n",
    "            source_entity = record[\"a\"]\n",
    "            relationships = record[\"r\"]\n",
    "\n",
    "            # Format the extracted information into a string or structured data\n",
    "            # suitable for your RAG model\n",
    "\n",
    "            relationships_txt_list = [f\" {relationship.type} {get_node_text(relationship.nodes[0])}\" for relationship in relationships]\n",
    "            context_string = f\"{get_node_text(source_entity)} {\" \".join(relationships_txt_list)}\"\n",
    "\n",
    "            #print(f\"Context string : {context_string}\")\n",
    "            \n",
    "            context_list.append(context_string)\n",
    "        return \"\\n\".join(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7628eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_copy_graph_query_result(session, record):\n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a node\n",
    "\n",
    "        if hasattr(value, 'labels'):\n",
    "            props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(value).items())\n",
    "            node_query = f\"\"\"\n",
    "                MERGE (n:TempViz:{re.sub(r\"\\s+\", \"_\", list(value.labels)[0])} {{element_id:\"{value.element_id}\"}})\n",
    "                SET n += {{{props}}}\n",
    "            \"\"\"\n",
    "            session.run(node_query)\n",
    "\n",
    "            \n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a relationship\n",
    "        if hasattr(value, 'type'):\n",
    "            start_id = value.nodes[0].element_id\n",
    "            end_id = value.nodes[1].element_id\n",
    "\n",
    "            props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(value).items())\n",
    "            rel_query = f\"\"\"\n",
    "                MATCH (a:TempViz {{element_id:\"{start_id}\"}}), (b:TempViz {{element_id:\"{end_id}\"}})\n",
    "                MERGE (a)-[r:{value.type}]->(b)\n",
    "                SET r += {{{props}}}\n",
    "            \"\"\"\n",
    "            session.run(rel_query)\n",
    "\n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a list of relationships\n",
    "        if isinstance(value, list):\n",
    "\n",
    "            for rel in value:\n",
    "\n",
    "                start_id = rel.nodes[0].element_id\n",
    "                end_id = rel.nodes[1].element_id\n",
    "\n",
    "                props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(rel).items())\n",
    "                rel_query = f\"\"\"\n",
    "                    MATCH (a:TempViz {{element_id:\"{start_id}\"}}), (b:TempViz {{element_id:\"{end_id}\"}})\n",
    "                    MERGE (a)-[r:{rel.type}]->(b)\n",
    "                    SET r += {{{props}}}\n",
    "                \"\"\"\n",
    "                session.run(rel_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c30dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result_with_yfiles(result):\n",
    "    \"\"\"\n",
    "    Push nodes/relationships from a neo4j.Result to a temporary in-memory graph\n",
    "    and display them with Neo4jGraphWidget.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    with driver.session() as session:\n",
    "        # Clear a temporary namespace (label:TempViz)\n",
    "        session.run(\"MATCH (n:TempViz) DETACH DELETE n\")\n",
    "        \n",
    "        for record in result:\n",
    "            #print(f\"Record : {record}\")\n",
    "            temp_copy_graph_query_result(session, record)\n",
    "\n",
    "    # Now visualize just the TempViz graph\n",
    "    widget = Neo4jGraphWidget(driver)\n",
    "    widget.show_cypher(\"\"\"\n",
    "        MATCH (n:TempViz)-[r]->(m:TempViz)\n",
    "        RETURN n, r, m\n",
    "    \"\"\",layout=\"hierarchic\")\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81b69c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_graph_with_yfiles():\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))  \n",
    "    neo4j_subgraph=Neo4jGraphWidget(driver)\n",
    "\n",
    "    neo4j_subgraph.show_cypher(\"MATCH (s)-[r]->(t) RETURN s,r,t LIMIT 40\", layout=\"hierarchic\")\n",
    "    driver.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec729b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_mllm_img_desc(img_path):\n",
    "    img_b64 = encode_image(img_path)\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Describe all people, organizations, and events in this image.\",\n",
    "                \"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    image_description = response['message']['content']\n",
    "    return image_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0abb81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a Cypher query\n",
    "def run_query(query, parameters=None):\n",
    "    # Create a driver instance\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    tobereturned = []\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters or {})\n",
    "        tobereturned =  [record for record in result]\n",
    "    driver.close()\n",
    "    return tobereturned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_from_txt_with_rag_context(search, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    # Get RAG context\n",
    "    query = f\"\"\"\n",
    "    MATCH (a)-[r*1..{max_graph_depth}]->(b)\n",
    "    WITH a, b, gds.similarity.cosine(a.embedding, $query_embeddings) AS similarity, r\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT $top_k\n",
    "    RETURN a, r, b\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"query_embeddings\": get_text_embedding(search), \"top_k\":num_top_results}\n",
    "    rag_subgraph = run_query(query, params)\n",
    "\n",
    "    # Print results\n",
    "    #for record in results:\n",
    "        #print(record)\n",
    "    # Format RAG context\n",
    "    search_ctx = format_context_for_rag(rag_subgraph)\n",
    "\n",
    "    # Query llm with rag context\n",
    "    return main_mllm_txt_search(search,search_ctx), rag_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4d2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_from_img_with_rag_context(img_path, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    instruction = \"What is depicted in this image ?\"\n",
    "    img_txt = get_query_mllm_img_desc(instruction, img_path)\n",
    "    \n",
    "    return search_from_txt_with_rag_context(img_txt, max_graph_depth, num_top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ce76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text_docs, img_data):\n",
    "    \n",
    "    # Preprocess textual data\n",
    "    docs = [Document(page_content=txt, metadata={\"embedding\":get_text_embedding(txt)}) for txt in text_docs[RAG_DATA_DOC_COL_NAME]]\n",
    "\n",
    "\n",
    "    img_docs = [Document(page_content=img_desc, metadata={\"url\": get_rag_img_path(img_name), \"embedding\":get_text_embedding(img_desc)}) \n",
    "                for img_desc, img_name in zip(img_data[RAG_DATA_DOC_COL_NAME], img_data[RAG_DATA_IMG_COL_NAME])]\n",
    "\n",
    "    \n",
    "    # Gather all preprocessed data\n",
    "    docs.extend(img_docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8100ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_graph_post_treatment(graph_docs):\n",
    "    \n",
    "    # Add embeddings to the nodes of the graph\n",
    "    for graph_doc in graph_docs:\n",
    "        \n",
    "        for node in graph_doc.nodes:\n",
    "            node_text = node.properties.get(\"text\")\n",
    "            if node_text is not None :\n",
    "                node.properties[\"embedding\"] = get_text_embedding(node_text)\n",
    "            else : \n",
    "                node_id = node.id\n",
    "                if node_id is not None :\n",
    "                    node.properties[\"embedding\"] = get_text_embedding(node_id)\n",
    "\n",
    "                \n",
    "    # Add Image nodes with properties like embedding then relate to the rest of the graph\n",
    "    img_id = 0\n",
    "\n",
    "    for graph_doc in graph_docs:\n",
    "        # If the source of the graphDoc is the description of an image\n",
    "        graph_source = graph_doc.source\n",
    "        img_url = graph_source.metadata.get(\"url\") \n",
    "        \n",
    "        if img_url is not None:   \n",
    "            #print(img_url) \n",
    "            # Créer un noeud image avec l'URL en question et l'embedding de l'image\n",
    "            img_node = Node(id=f\"img_{img_id}\", type=\"Image\", properties={\"url\": img_url, \"embedding\":get_img_embedding(img_url)})   \n",
    "            \n",
    "            new_relationships = []\n",
    "\n",
    "            # Lier l'image à tous les noeuds du grapheDoc\n",
    "            for node in graph_doc.nodes:\n",
    "                new_relationships.append(Relationship(source=img_node,target=node, type=\"contains\"))\n",
    "            \n",
    "            graph_doc.nodes.append(img_node)\n",
    "            graph_doc.relationships.extend(new_relationships)\n",
    "            img_id += 1\n",
    "\n",
    "    return graph_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac4a781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_neo4j_graph():\n",
    "    \n",
    "    \n",
    "    raw_data_df = pd.read_csv(RAG_DATA_FILENAME,sep=\",\")\n",
    "\n",
    "    # Preprocess all data\n",
    "    docs = preprocess_data(raw_data_df[raw_data_df[RAG_DATA_IMG_COL_NAME].isna()], raw_data_df[ raw_data_df[RAG_DATA_IMG_COL_NAME].notna()])\n",
    "\n",
    "    # Normal Ollama LLM for graph extraction\n",
    "    llm = OllamaLLM(model=MULTIMODAL_INFERENCE_MODEL, temperature=0.0)\n",
    "\n",
    "    transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        #allowed_nodes=[\"Person\", \"Organization\", \"Event\"],\n",
    "        #node_properties=True\n",
    "    )\n",
    "    \n",
    "    # 3. Extract graph\n",
    "    graph_docs = transformer.convert_to_graph_documents(docs)\n",
    "    graph_docs = extracted_graph_post_treatment(graph_docs)\n",
    "\n",
    "    #Empty graph \n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    with driver.session() as session:\n",
    "\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    driver.close()\n",
    "    \n",
    "    # Store Knowledge Graph in Neo4j\n",
    "    graph_store = Neo4jGraph(url=NEO4J_SERVER_URL, username=NEO4J_LOGIN, password=NEO4J_PWD, database=NEO4J_DB_NAME)\n",
    "    #graph_store.write_graph(graph_docs)\n",
    "\n",
    "    graph_store.add_graph_documents(graph_docs, include_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df4ddfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrdiouf\\AppData\\Local\\Temp\\ipykernel_17420\\1350047584.py:30: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph_store = Neo4jGraph(url=NEO4J_SERVER_URL, username=NEO4J_LOGIN, password=NEO4J_PWD, database=NEO4J_DB_NAME)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    populate_neo4j_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4925367",
   "metadata": {},
   "source": [
    "## Requête incluant le RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image shows two individuals standing next to each other. Both appear to be happy or smiling and are facing the camera with one person slightly ahead of the other. They seem to be outside, as indicated by the natural light and the presence of a building in the background. There is a watermark at the bottom indicating that this is a watermark image, which suggests it may have been sourced from a website or used for promotional purposes. \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    # Query 1\n",
    "    search1 = \"Where does Alice work?\"\n",
    "\n",
    "    # Query 2\n",
    "    search2 = \"Who works for OpenWidgets?\"\n",
    "\n",
    "    search3 = \"black\"\n",
    "    search4 = \"describe happy people\"\n",
    "\n",
    "    search_result, rag_subgraph = search_from_txt_with_rag_context(search4, max_graph_depth=RAG_MAX_GRAPH_DEPTH, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)\n",
    "    file_path = get_user_img_search_path('Man.jpg') # Replace with the actual path to your image\n",
    "    img_search_result, img_rag_subgraph = search_from_img_with_rag_context(file_path,max_graph_depth=RAG_MAX_GRAPH_DEPTH, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69001b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's analyze this context and describe happy people.\n",
      "\n",
      "Given the information:\n",
      "\n",
      "*   **people WEARING people:** This suggests a sense of connection, perhaps empathy, understanding, or even playful interaction between individuals. It implies an awareness of and engagement with the experiences of others.\n",
      "*   **people HAS_ATTRIBUTE people:** This reinforces that \"people\" is a fundamental category with inherent qualities. It highlights that people *are* defined by their humanity, their capacity for feeling, and their existence.\n",
      "*   **people WEARING people:** This emphasizes the ability to connect with and understand others, and likely a feeling of compassion or caring.\n",
      "\n",
      "Therefore, based on this context, **happy people are likely those who deeply connect with others and demonstrate empathy and care.** They aren't just superficially cheerful; they possess a genuine sense of connection and understanding within the broader human experience.  They likely *wear* others' perspectives on their faces - radiating happiness from a place of kindness and recognition.\n",
      "\n",
      "**In short, happy people are characterized by their ability to relate to and appreciate the experiences of others.**\n",
      "\n",
      "Would you like me to elaborate on any specific aspect of this description, such as the *why* behind this connection, or explore different facets of happiness?\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ba87609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided description, the image depicts a smiling man with a dark complexion, dark hair styled in a low fade and a prominent dark beard. He’s wearing a dark-colored shirt and is positioned in an urban setting – likely a street scene – with a building featuring a textured facade and some greenery in the background. The lighting is soft and diffused due to an overcast day.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    print(img_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79e3cdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e07168e00d3473a91dcae57e105edb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(rag_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3604882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e3b663deaf44a9a3742383074874c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(img_rag_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1daf651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cc03aec2cb4423b1e457fd474b28e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='750px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    visualize_all_graph_with_yfiles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIT_DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
