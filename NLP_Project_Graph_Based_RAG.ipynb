{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11d2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomic\n",
    "from nomic import embed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import ollama\n",
    "from ollama import chat\n",
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from yfiles_jupyter_graphs_for_neo4j import Neo4jGraphWidget\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b52d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import deeplake\n",
    "#ds = deeplake.load('hub://activeloop/flickr30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c297a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_SERVER_URL = \"bolt://localhost:7687\"\n",
    "NEO4J_DB_NAME= \"ragdb\"\n",
    "NEO4J_LOGIN = os.environ['NEO4J_USER_LOGIN']\n",
    "NEO4J_PWD = os.environ['NEO4J_USER_PWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7139e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL = 'DC1LEX/nomic-embed-text-v1.5-multimodal'\n",
    "VISION_EMBEDDING_MODEL = 'nomic-embed-vision-v1.5'\n",
    "MULTIMODAL_INFERENCE_MODEL = \"gemma3:4b\"\n",
    "MULTIMODAL_LLAVA_MODEL = \"llava:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ea2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DATA_FILENAME = \"rag_data.csv\"\n",
    "RAG_DATA_IMG_COL_NAME = \"image_filename\"\n",
    "RAG_DATA_DOC_COL_NAME = \"doc\"\n",
    "RAG_IMG_FOLDER = \"images\"\n",
    "RAG_MAX_GRAPH_DEPTH = 2\n",
    "RAG_QUERY_NUM_TOP_RESULTS = 6\n",
    "USER_QUERY_IMAGE_SEARCH_FOLDER = \"user_image_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c954e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_img_path(filename):\n",
    "    local_path = f\"{RAG_IMG_FOLDER}/{filename}\"\n",
    "    if os.path.isfile(local_path):\n",
    "        return local_path\n",
    "    else :\n",
    "        print(f\"File {local_path} does not exist or is not a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5791532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_img_search_path(filename):\n",
    "    local_path = f\"{USER_QUERY_IMAGE_SEARCH_FOLDER}/{filename}\"\n",
    "    if os.path.isfile(local_path):\n",
    "        return local_path\n",
    "    else :\n",
    "        print(f\"File {local_path} does not exist or is not a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293df4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img_path):\n",
    "    encoded_image = None\n",
    "\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        encoded_image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    return encoded_image\n",
    "\n",
    "def encode_images(img_paths):\n",
    "    return [encode_image(img_path) for img_path in img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060a09c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(TEXT_EMBEDDING_MODEL)\n",
    "ollama.pull(MULTIMODAL_INFERENCE_MODEL)\n",
    "ollama.pull(MULTIMODAL_LLAVA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5311c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_embedding(img_path):\n",
    "    nomic_api_key = os.environ['NOMIC_API_KEY'] \n",
    "\n",
    "    nomic.login(nomic_api_key)\n",
    "\n",
    "    output = embed.image(\n",
    "        images=[img_path\n",
    "        ],\n",
    "        model=VISION_EMBEDDING_MODEL,\n",
    "    )\n",
    "\n",
    "    #print(output['usage'])\n",
    "    img_embeddings = np.array(output['embeddings'])\n",
    "    #print(img_embeddings)\n",
    "    #print(img_embeddings.shape)\n",
    "    \n",
    "    return img_embeddings[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77cc7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(txt):\n",
    "    response = ollama.embed(model=TEXT_EMBEDDING_MODEL, input=txt)\n",
    "    txt_embedding = response[\"embeddings\"]\n",
    "    return txt_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5141d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_from_text_prop(graph_doc, txt):\n",
    "    #result = None\n",
    "    for node in graph_doc.nodes:\n",
    "        txt_value = node.properties.get(\"text\")\n",
    "        if txt_value is not None and txt_value == txt:\n",
    "            return node\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21721f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mllm_txt_search(txt_search, ctx):\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Answer the question while taking into account the context. \\n question:{txt_search} \\n context:{ctx}\",\n",
    "                #\"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = response['message']['content']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b483f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_mllm_img_desc(instruction, file_path):\n",
    "    try:\n",
    "        # Ensure the image exists and is accessible\n",
    "        with open(file_path, 'rb') as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "        result = ollama.generate(\n",
    "            model=MULTIMODAL_LLAVA_MODEL,\n",
    "            prompt=instruction,\n",
    "            images=[image_data], # Pass image data directly\n",
    "            stream=False\n",
    "        )\n",
    "        #print(result['response'])\n",
    "        return result['response']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab8737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_text(node):\n",
    "    node_txt = node.get(\"text\")\n",
    "    if node_txt is not None :\n",
    "        return node_txt\n",
    "    else:\n",
    "        return node.get(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04149aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_rag(neo4j_results):\n",
    "    context_list = []\n",
    "    for record in neo4j_results:\n",
    "        # Extract relevant information from the record\n",
    "        source_entity = record[\"a\"]\n",
    "        relationships = record[\"r\"]\n",
    "\n",
    "        # Format the extracted information into a string or structured data\n",
    "        # suitable for your RAG model\n",
    "\n",
    "        relationships_txt_list = [f\" {relationship.type} {get_node_text(relationship.nodes[0])}\" for relationship in relationships]\n",
    "        context_string = f\"{get_node_text(source_entity)} {\" \".join(relationships_txt_list)}\"\n",
    "\n",
    "        #print(f\"Context string : {context_string}\")\n",
    "        \n",
    "        context_list.append(context_string)\n",
    "    return \"\\n\".join(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7628eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_copy_graph_query_result(session, record):\n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a node\n",
    "\n",
    "        if hasattr(value, 'labels'):\n",
    "            props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(value).items())\n",
    "            node_query = f\"\"\"\n",
    "                MERGE (n:TempViz:{re.sub(r\"\\s+\", \"_\", list(value.labels)[0])} {{element_id:\"{value.element_id}\"}})\n",
    "                SET n += {{{props}}}\n",
    "            \"\"\"\n",
    "            session.run(node_query)\n",
    "\n",
    "            \n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a relationship\n",
    "        if hasattr(value, 'type'):\n",
    "            start_id = value.nodes[0].element_id\n",
    "            end_id = value.nodes[1].element_id\n",
    "            #print(\"simple relationship\")\n",
    "            props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(value).items())\n",
    "            rel_query = f\"\"\"\n",
    "                MATCH (a:TempViz {{element_id:\"{start_id}\"}}), (b:TempViz {{element_id:\"{end_id}\"}})\n",
    "                MERGE (a)-[r:{value.type}]->(b)\n",
    "                SET r += {{{props}}}\n",
    "            \"\"\"\n",
    "            session.run(rel_query)\n",
    "\n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a list of relationships\n",
    "        if isinstance(value, list):\n",
    "            #print(\"list relationship\")\n",
    "            #print(f\"Rels : {value}\")\n",
    "            for rel in value:\n",
    "                \n",
    "                start_id = rel.nodes[0].element_id\n",
    "                end_id = rel.nodes[1].element_id\n",
    "                #print(f\"Rel : {rel}\")\n",
    "                #print(f\"Rel start id : {start_id}\")\n",
    "                #print(f\"Rel end id : {end_id}\")\n",
    "                props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(rel).items())\n",
    "                rel_query = f\"\"\"\n",
    "                    MATCH (a:TempViz {{element_id:\"{start_id}\"}}), (b:TempViz {{element_id:\"{end_id}\"}})\n",
    "                    MERGE (a)-[r:{rel.type}]->(b)\n",
    "                    SET r += {{{props}}}\n",
    "                \"\"\"\n",
    "                session.run(rel_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c30dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result_with_yfiles(result):\n",
    "    \"\"\"\n",
    "    Push nodes/relationships from a neo4j.Result to a temporary in-memory graph\n",
    "    and display them with Neo4jGraphWidget.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    with driver.session() as session:\n",
    "        # Clear a temporary namespace (label:TempViz)\n",
    "        session.run(\"MATCH (n:TempViz) DETACH DELETE n\")\n",
    "        \n",
    "        for record in result:\n",
    "            #print(f\"Record : {record}\")\n",
    "            temp_copy_graph_query_result(session, record)\n",
    "\n",
    "    # Now visualize just the TempViz graph\n",
    "    widget = Neo4jGraphWidget(driver)\n",
    "    widget.show_cypher(\"\"\"\n",
    "        MATCH (n:TempViz)-[r]->(m:TempViz)\n",
    "        RETURN n, r, m\n",
    "    \"\"\",layout=\"hierarchic\")\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81b69c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_graph_with_yfiles():\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))  \n",
    "    neo4j_subgraph=Neo4jGraphWidget(driver)\n",
    "\n",
    "    neo4j_subgraph.show_cypher(\"MATCH (s)-[r]->(t) RETURN s,r,t LIMIT 40\", layout=\"hierarchic\")\n",
    "    driver.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec729b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_mllm_img_desc(img_path):\n",
    "    img_b64 = encode_image(img_path)\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Describe all people, organizations, and events in this image.\",\n",
    "                \"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    image_description = response['message']['content']\n",
    "    return image_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0abb81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a Cypher query\n",
    "def run_query(query, parameters=None):\n",
    "    # Create a driver instance\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    tobereturned = []\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters or {})\n",
    "        tobereturned =  [record for record in result]\n",
    "    driver.close()\n",
    "    return tobereturned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "838972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_from_txt_with_rag_context(search, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    # Get RAG context\n",
    "    query = f\"\"\"\n",
    "    MATCH (a)-[r*1..{max_graph_depth}]->(b)\n",
    "    WITH a, b, gds.similarity.cosine(a.embedding, $query_embeddings) AS similarity, r\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT $top_k\n",
    "    RETURN a, r, b\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"query_embeddings\": get_text_embedding(search), \"top_k\":num_top_results}\n",
    "    rag_subgraph = run_query(query, params)\n",
    "\n",
    "    # Print results\n",
    "    #for record in results:\n",
    "        #print(record)\n",
    "    # Format RAG context\n",
    "    search_ctx = format_context_for_rag(rag_subgraph)\n",
    "\n",
    "    # Query llm with rag context\n",
    "    return main_mllm_txt_search(search,search_ctx), rag_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4d2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_from_img_with_rag_context(img_path, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    instruction = \"What is depicted in this image ?\"\n",
    "    img_txt = get_query_mllm_img_desc(instruction, img_path)\n",
    "    #print(\"Image search\")\n",
    "    return search_from_txt_with_rag_context(img_txt, max_graph_depth, num_top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e1ce76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text_docs, img_data):\n",
    "    \n",
    "    # Preprocess textual data\n",
    "    docs = [Document(page_content=txt, metadata={\"embedding\":get_text_embedding(txt)}) for txt in text_docs[RAG_DATA_DOC_COL_NAME]]\n",
    "\n",
    "\n",
    "    img_docs = [Document(page_content=img_desc, metadata={\"url\": get_rag_img_path(img_name), \"embedding\":get_text_embedding(img_desc)}) \n",
    "                for img_desc, img_name in zip(img_data[RAG_DATA_DOC_COL_NAME], img_data[RAG_DATA_IMG_COL_NAME])]\n",
    "\n",
    "    \n",
    "    # Gather all preprocessed data\n",
    "    docs.extend(img_docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8100ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_graph_post_treatment(graph_docs):\n",
    "    \n",
    "    # Add embeddings to the nodes of the graph\n",
    "    for graph_doc in graph_docs:\n",
    "        \n",
    "        for node in graph_doc.nodes:\n",
    "            node_text = node.properties.get(\"text\")\n",
    "            if node_text is not None :\n",
    "                node.properties[\"embedding\"] = get_text_embedding(node_text)\n",
    "            else : \n",
    "                node_id = node.id\n",
    "                if node_id is not None :\n",
    "                    node.properties[\"embedding\"] = get_text_embedding(node_id)\n",
    "\n",
    "                \n",
    "    # Add Image nodes with properties like embedding then relate to the rest of the graph\n",
    "    img_id = 0\n",
    "\n",
    "    for graph_doc in graph_docs:\n",
    "        # If the source of the graphDoc is the description of an image\n",
    "        graph_source = graph_doc.source\n",
    "        img_url = graph_source.metadata.get(\"url\") \n",
    "        \n",
    "        if img_url is not None:   \n",
    "            #print(img_url) \n",
    "            # Créer un noeud image avec l'URL en question et l'embedding de l'image\n",
    "            img_node = Node(id=f\"img_{img_id}\", type=\"Image\", properties={\"url\": img_url, \"embedding\":get_img_embedding(img_url)})   \n",
    "            \n",
    "            new_relationships = []\n",
    "\n",
    "            # Lier l'image à tous les noeuds du grapheDoc\n",
    "            for node in graph_doc.nodes:\n",
    "                new_relationships.append(Relationship(source=img_node,target=node, type=\"contains\"))\n",
    "            \n",
    "            graph_doc.nodes.append(img_node)\n",
    "            graph_doc.relationships.extend(new_relationships)\n",
    "            img_id += 1\n",
    "\n",
    "    return graph_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac4a781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_neo4j_graph():\n",
    "    \n",
    "    \n",
    "    raw_data_df = pd.read_csv(RAG_DATA_FILENAME,sep=\",\")\n",
    "\n",
    "    # Preprocess all data\n",
    "    docs = preprocess_data(raw_data_df[raw_data_df[RAG_DATA_IMG_COL_NAME].isna()], raw_data_df[ raw_data_df[RAG_DATA_IMG_COL_NAME].notna()])\n",
    "\n",
    "    # Normal Ollama LLM for graph extraction\n",
    "    llm = OllamaLLM(model=MULTIMODAL_INFERENCE_MODEL, temperature=0.0)\n",
    "\n",
    "    transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        #allowed_nodes=[\"Person\", \"Organization\", \"Event\"],\n",
    "        #node_properties=True\n",
    "    )\n",
    "    \n",
    "    # 3. Extract graph\n",
    "    graph_docs = transformer.convert_to_graph_documents(docs)\n",
    "    graph_docs = extracted_graph_post_treatment(graph_docs)\n",
    "\n",
    "    #Empty graph \n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    with driver.session() as session:\n",
    "\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    driver.close()\n",
    "    \n",
    "    # Store Knowledge Graph in Neo4j\n",
    "    graph_store = Neo4jGraph(url=NEO4J_SERVER_URL, username=NEO4J_LOGIN, password=NEO4J_PWD, database=NEO4J_DB_NAME)\n",
    "    #graph_store.write_graph(graph_docs)\n",
    "\n",
    "    graph_store.add_graph_documents(graph_docs, include_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ddfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    populate_neo4j_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4925367",
   "metadata": {},
   "source": [
    "## Requête incluant le RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    # Query 1\n",
    "    search1 = \"Where does Alice work?\"\n",
    "\n",
    "    # Query 2\n",
    "    search2 = \"Who works for OpenWidgets?\"\n",
    "\n",
    "    search3 = \"black\"\n",
    "    search4 = \"describe happy people\"\n",
    "\n",
    "    search_result, rag_subgraph = search_from_txt_with_rag_context(search4, max_graph_depth=RAG_MAX_GRAPH_DEPTH, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)\n",
    "    file_path = get_user_img_search_path('Man.jpg') # Replace with the actual path to your image\n",
    "    img_search_result, img_rag_subgraph = search_from_img_with_rag_context(file_path,max_graph_depth=RAG_MAX_GRAPH_DEPTH, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69001b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ba87609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, based on the provided descriptions, here’s a breakdown of the two men in the image:\n",
      "\n",
      "*   **Man on the Left:** He is wearing a denim jacket and has a smiling expression.\n",
      "*   **Man on the Right:** He is wearing a T-shirt and has a neutral expression.\n",
      "\n",
      "Both men are in an urban setting – a street scene with a building and greenery in the background, under overcast lighting. The man on the right is a dark-skinned man with dark hair styled in a low fade and a prominent dark beard.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    print(img_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(rag_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3604882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610cadcfb74b482388a9c5486a031752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(img_rag_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1daf651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44750651150f4cb0a2728e2604f384c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='770px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    visualize_all_graph_with_yfiles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIT_DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
