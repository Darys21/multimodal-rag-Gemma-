{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11d2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomic\n",
    "from nomic import embed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import ollama\n",
    "from ollama import chat\n",
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from yfiles_jupyter_graphs_for_neo4j import Neo4jGraphWidget\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c297a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_SERVER_URL = \"bolt://localhost:7687\"\n",
    "NEO4J_DB_NAME= \"ragdb\"\n",
    "NEO4J_LOGIN = os.environ['NEO4J_USER_LOGIN']\n",
    "NEO4J_PWD = os.environ['NEO4J_USER_PWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7139e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL = 'DC1LEX/nomic-embed-text-v1.5-multimodal'\n",
    "VISION_EMBEDDING_MODEL = 'nomic-embed-vision-v1.5'\n",
    "MULTIMODAL_INFERENCE_MODEL = \"gemma3:4b\"\n",
    "MULTIMODAL_LLAVA_MODEL = \"llava:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ea2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DATA_FILENAME = \"rag_data.csv\"\n",
    "RAG_DATA_IMG_COL_NAME = \"image_filename\"\n",
    "RAG_DATA_DOC_COL_NAME = \"doc\"\n",
    "RAG_IMG_FOLDER = \"images\"\n",
    "RAG_MAX_GRAPH_DEPTH = 2\n",
    "RAG_QUERY_NUM_TOP_RESULTS = 6\n",
    "USER_QUERY_IMAGE_SEARCH_FOLDER = \"user_image_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ea73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCH_DATA_IMG_COL_NAME = \"image\"\n",
    "BENCH_DATA_GT_IMG_COL_NAME = \"gt_images\"\n",
    "BENCH_DATA_QUESTION_COL_NAME = \"question\"\n",
    "BENCH_DATA_CHOICE_A_COL_NAME = \"A\"\n",
    "BENCH_DATA_CHOICE_B_COL_NAME = \"B\"\n",
    "BENCH_DATA_CHOICE_C_COL_NAME = \"C\"\n",
    "BENCH_DATA_CHOICE_D_COL_NAME = \"D\"\n",
    "BENCH_DATA_ANSWER_CHOICE_COL_NAME = \"answer_choice\"\n",
    "BENCH_DATA_ANSWER_TEXT_COL_NAME = \"answer\"\n",
    "BENCH_DATA_BASE_SUBSET_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c5cbc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(TEXT_EMBEDDING_MODEL)\n",
    "ollama.pull(MULTIMODAL_INFERENCE_MODEL)\n",
    "ollama.pull(MULTIMODAL_LLAVA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c954e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_img_path(filename):\n",
    "    local_path = os.path.join(RAG_IMG_FOLDER,filename)\n",
    "    if os.path.isfile(local_path):\n",
    "        return local_path\n",
    "    else :\n",
    "        print(f\"File {local_path} does not exist or is not a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5791532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_img_search_path(filename):\n",
    "    local_path = f\"{USER_QUERY_IMAGE_SEARCH_FOLDER}/{filename}\"\n",
    "    if os.path.isfile(local_path):\n",
    "        return local_path\n",
    "    else :\n",
    "        print(f\"File {local_path} does not exist or is not a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293df4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img_path):\n",
    "    encoded_image = None\n",
    "\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        encoded_image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    return encoded_image\n",
    "\n",
    "def encode_images(img_paths):\n",
    "    return [encode_image(img_path) for img_path in img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d3d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_to_folder(folder,image_data):\n",
    "    img_name = f\"{uuid.uuid4()}.png\"\n",
    "            \n",
    "    img_path = os.path.join(folder, img_name)\n",
    "    \n",
    "    image_data.save(img_path)\n",
    "\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ce985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query_question(base_question, option_a, option_b, option_c, option_d):\n",
    "    return f\"\"\"{base_question}.\n",
    "    Simply state the answer you choose among the following options :\n",
    "    - {option_a}\n",
    "    - {option_b}\n",
    "    - {option_c}\n",
    "    - {option_d}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5311c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_embedding(img_path):\n",
    "    nomic_api_key = os.environ['NOMIC_API_KEY'] \n",
    "\n",
    "    nomic.login(nomic_api_key)\n",
    "\n",
    "    output = embed.image(\n",
    "        images=[img_path\n",
    "        ],\n",
    "        model=VISION_EMBEDDING_MODEL,\n",
    "    )\n",
    "\n",
    "    #print(output['usage'])\n",
    "    img_embeddings = np.array(output['embeddings'])\n",
    "    #print(img_embeddings)\n",
    "    #print(img_embeddings.shape)\n",
    "    \n",
    "    return img_embeddings[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77cc7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(txt):\n",
    "    response = ollama.embed(model=TEXT_EMBEDDING_MODEL, input=txt)\n",
    "    txt_embedding = response[\"embeddings\"]\n",
    "    return txt_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5141d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_from_text_prop(graph_doc, txt):\n",
    "    #result = None\n",
    "    for node in graph_doc.nodes:\n",
    "        txt_value = node.properties.get(\"text\")\n",
    "        if txt_value is not None and txt_value == txt:\n",
    "            return node\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21721f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mllm_txt_search(txt_search, ctx):\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Answer the question while taking into account the context. \\n question:{txt_search} \\n context:{ctx}\",\n",
    "                #\"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = response['message']['content']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b483f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_mllm_img_desc(instruction, file_path):\n",
    "    try:\n",
    "        # Ensure the image exists and is accessible\n",
    "        with open(file_path, 'rb') as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "        result = ollama.generate(\n",
    "            model=MULTIMODAL_LLAVA_MODEL,\n",
    "            prompt=instruction,\n",
    "            images=[image_data], # Pass image data directly\n",
    "            stream=False\n",
    "        )\n",
    "        #print(result['response'])\n",
    "        return result['response']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab8737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_text(node):\n",
    "    node_txt = node.get(\"text\")\n",
    "    if node_txt is not None :\n",
    "        return node_txt\n",
    "    else:\n",
    "        return node.get(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04149aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_rag(neo4j_results):\n",
    "    context_list = []\n",
    "    for record in neo4j_results:\n",
    "        # Extract relevant information from the record\n",
    "        source_entity = record[\"a\"]\n",
    "        relationships = record[\"r\"]\n",
    "\n",
    "        # Format the extracted information into a string or structured data\n",
    "        # suitable for your RAG model\n",
    "\n",
    "        relationships_txt_list = [f\" {relationship.type} {get_node_text(relationship.nodes[0])}\" for relationship in relationships]\n",
    "        context_string = f\"{get_node_text(source_entity)} {\" \".join(relationships_txt_list)}\"\n",
    "\n",
    "        #print(f\"Context string : {context_string}\")\n",
    "        \n",
    "        context_list.append(context_string)\n",
    "    return \"\\n\".join(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7628eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_copy_graph_query_result(session, record):\n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a node\n",
    "\n",
    "        if hasattr(value, 'labels'):\n",
    "            props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(value).items())\n",
    "            node_query = f\"\"\"\n",
    "                MERGE (n:TempViz:{re.sub(r\"\\s+\", \"_\", list(value.labels)[0])} {{element_id:\"{value.element_id}\"}})\n",
    "                SET n += {{{props}}}\n",
    "            \"\"\"\n",
    "            session.run(node_query)\n",
    "\n",
    "            \n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a relationship\n",
    "        if hasattr(value, 'type'):\n",
    "            start_id = value.nodes[0].element_id\n",
    "            end_id = value.nodes[1].element_id\n",
    "            #print(\"simple relationship\")\n",
    "            props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(value).items())\n",
    "            rel_query = f\"\"\"\n",
    "                MATCH (a:TempViz {{element_id:\"{start_id}\"}}), (b:TempViz {{element_id:\"{end_id}\"}})\n",
    "                MERGE (a)-[r:{value.type}]->(b)\n",
    "                SET r += {{{props}}}\n",
    "            \"\"\"\n",
    "            session.run(rel_query)\n",
    "\n",
    "    \n",
    "    for value in record.values():\n",
    "        # If it's a list of relationships\n",
    "        if isinstance(value, list):\n",
    "            #print(\"list relationship\")\n",
    "            #print(f\"Rels : {value}\")\n",
    "            for rel in value:\n",
    "                \n",
    "                start_id = rel.nodes[0].element_id\n",
    "                end_id = rel.nodes[1].element_id\n",
    "                #print(f\"Rel : {rel}\")\n",
    "                #print(f\"Rel start id : {start_id}\")\n",
    "                #print(f\"Rel end id : {end_id}\")\n",
    "                props = \", \".join(f\"{k}: {repr(v)}\" for k, v in dict(rel).items())\n",
    "                rel_query = f\"\"\"\n",
    "                    MATCH (a:TempViz {{element_id:\"{start_id}\"}}), (b:TempViz {{element_id:\"{end_id}\"}})\n",
    "                    MERGE (a)-[r:{rel.type}]->(b)\n",
    "                    SET r += {{{props}}}\n",
    "                \"\"\"\n",
    "                session.run(rel_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c30dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result_with_yfiles(result):\n",
    "    \"\"\"\n",
    "    Push nodes/relationships from a neo4j.Result to a temporary in-memory graph\n",
    "    and display them with Neo4jGraphWidget.\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    with driver.session() as session:\n",
    "        # Clear a temporary namespace (label:TempViz)\n",
    "        session.run(\"MATCH (n:TempViz) DETACH DELETE n\")\n",
    "        \n",
    "        for record in result:\n",
    "            #print(f\"Record : {record}\")\n",
    "            temp_copy_graph_query_result(session, record)\n",
    "\n",
    "    # Now visualize just the TempViz graph\n",
    "    widget = Neo4jGraphWidget(driver)\n",
    "    widget.show_cypher(\"\"\"\n",
    "        MATCH (n:TempViz)-[r]->(m:TempViz)\n",
    "        RETURN n, r, m\n",
    "    \"\"\",layout=\"hierarchic\")\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81b69c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_graph_with_yfiles():\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))  \n",
    "    neo4j_subgraph=Neo4jGraphWidget(driver)\n",
    "\n",
    "    neo4j_subgraph.show_cypher(\"MATCH (s)-[r]->(t) RETURN s,r,t LIMIT 40\", layout=\"hierarchic\")\n",
    "    driver.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec729b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_mllm_img_desc(img_path):\n",
    "    img_b64 = encode_image(img_path)\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Describe all people, organizations, and events in this image.\",\n",
    "                \"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    image_description = response['message']['content']\n",
    "    return image_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7579361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mllm_img_with_query_search(img_path, query):\n",
    "    img_b64 = encode_image(img_path)\n",
    "    response = chat(\n",
    "        model=MULTIMODAL_INFERENCE_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "                \"images\":[img_b64]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = response['message']['content']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0abb81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a Cypher query\n",
    "def run_query(query, parameters=None):\n",
    "    # Create a driver instance\n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    tobereturned = []\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters or {})\n",
    "        tobereturned =  [record for record in result]\n",
    "    driver.close()\n",
    "    return tobereturned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "838972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_from_txt_with_rag_context(search, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    # Get RAG context\n",
    "    query = f\"\"\"\n",
    "    MATCH (a)-[r*1..{max_graph_depth}]->(b)\n",
    "    WITH a, b, gds.similarity.cosine(a.embedding, $query_embeddings) AS similarity, r\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT $top_k\n",
    "    RETURN a, r, b\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"query_embeddings\": get_text_embedding(search), \"top_k\":num_top_results}\n",
    "    rag_subgraph = run_query(query, params)\n",
    "\n",
    "    # Print results\n",
    "    #for record in results:\n",
    "        #print(record)\n",
    "    # Format RAG context\n",
    "    search_ctx = format_context_for_rag(rag_subgraph)\n",
    "\n",
    "    # Query llm with rag context\n",
    "    return main_mllm_txt_search(search,search_ctx), rag_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb4d2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query_mllm_from_img_with_rag_context(img_path, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    instruction = \"What is depicted in this image ?\"\n",
    "    img_txt = get_query_mllm_img_desc(instruction, img_path)\n",
    "    #print(\"Image search\")\n",
    "    return search_from_txt_with_rag_context(img_txt, max_graph_depth, num_top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4093ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_main_mllm_from_img_with_query_and_rag_context(img_path, user_query, max_graph_depth=1, num_top_results=3):\n",
    "    \n",
    "    # Get RAG context\n",
    "    query = f\"\"\"\n",
    "    MATCH (a)-[r*1..{max_graph_depth}]->(b)\n",
    "    WITH a, b, gds.similarity.cosine(a.embedding, $query_embeddings) AS similarity, r\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT $top_k\n",
    "    RETURN a, r, b\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"query_embeddings\": get_img_embedding(img_path), \"top_k\":num_top_results}\n",
    "    rag_subgraph = run_query(query, params)\n",
    "\n",
    "    # Print results\n",
    "    #for record in results:\n",
    "        #print(record)\n",
    "    # Format RAG context with initial user query\n",
    "    search_ctx = f\"\"\"question : {user_query}\n",
    "                    context from image : {format_context_for_rag(rag_subgraph)}\"\"\"\n",
    "    \n",
    "    # Query llm with rag context\n",
    "    return main_mllm_img_with_query_search(img_path,search_ctx), rag_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e1ce76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text_docs, img_data):\n",
    "    \n",
    "    # Preprocess textual data\n",
    "    docs = [Document(page_content=txt, metadata={\"embedding\":get_text_embedding(txt)}) for txt in text_docs[RAG_DATA_DOC_COL_NAME]]\n",
    "\n",
    "\n",
    "    img_docs = [Document(page_content=img_desc, metadata={\"url\": get_rag_img_path(img_name), \"embedding\":get_text_embedding(img_desc)}) \n",
    "                for img_desc, img_name in zip(img_data[RAG_DATA_DOC_COL_NAME], img_data[RAG_DATA_IMG_COL_NAME])]\n",
    "\n",
    "    \n",
    "    # Gather all preprocessed data\n",
    "    docs.extend(img_docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccb86608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_benchmark_data(img_data):\n",
    "    \n",
    "    # Preprocess textual data\n",
    "    #docs = [Document(page_content=txt, metadata={\"embedding\":get_text_embedding(txt)}) for txt in text_docs[RAG_DATA_DOC_COL_NAME]]\n",
    "    img_docs = []\n",
    "    rec_num = 1\n",
    "    for imgs in img_data:\n",
    "        print(f\"Record num : {rec_num}\")\n",
    "        print(f\"Num ground truth images : {len(imgs)}\")\n",
    "        #There are 5 ground truth images, but we limit to 3 for the sake of computation limitation\n",
    "        for img in imgs[:3]:\n",
    "            \n",
    "            img_path = save_img_to_folder(RAG_IMG_FOLDER, img)            \n",
    " \n",
    "            img_desc = get_main_mllm_img_desc(img_path)\n",
    "            img_docs.append(Document(page_content=img_desc, metadata={\"url\": img_path, \"embedding\":get_text_embedding(img_desc)}))\n",
    "        \n",
    "        rec_num += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    # Gather all preprocessed data\n",
    "    #docs.extend(img_docs)\n",
    "\n",
    "    return img_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8100ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_graph_post_treatment(graph_docs):\n",
    "    \n",
    "    # Add embeddings to the nodes of the graph\n",
    "    for graph_doc in graph_docs:\n",
    "        \n",
    "        for node in graph_doc.nodes:\n",
    "            node_text = node.properties.get(\"text\")\n",
    "            if node_text is not None :\n",
    "                node.properties[\"embedding\"] = get_text_embedding(node_text)\n",
    "            else : \n",
    "                node_id = node.id\n",
    "                if node_id is not None :\n",
    "                    node.properties[\"embedding\"] = get_text_embedding(node_id)\n",
    "\n",
    "                \n",
    "    # Add Image nodes with properties like embedding then relate to the rest of the graph\n",
    "    img_id = 0\n",
    "\n",
    "    for graph_doc in graph_docs:\n",
    "        # If the source of the graphDoc is the description of an image\n",
    "        graph_source = graph_doc.source\n",
    "        img_url = graph_source.metadata.get(\"url\") \n",
    "        \n",
    "        if img_url is not None:   \n",
    "            #print(img_url) \n",
    "            # Créer un noeud image avec l'URL en question et l'embedding de l'image\n",
    "            img_node = Node(id=f\"img_{uuid.uuid4()}\", type=\"Image\", properties={\"url\": img_url, \"embedding\":get_img_embedding(img_url)})   \n",
    "            \n",
    "            new_relationships = []\n",
    "\n",
    "            # Lier l'image à tous les noeuds du grapheDoc\n",
    "            for node in graph_doc.nodes:\n",
    "                new_relationships.append(Relationship(source=img_node,target=node, type=\"contains\"))\n",
    "            \n",
    "            graph_doc.nodes.append(img_node)\n",
    "            graph_doc.relationships.extend(new_relationships)\n",
    "            img_id += 1\n",
    "\n",
    "    return graph_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac4a781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_neo4j_graph():\n",
    "    \n",
    "    \n",
    "    raw_data_df = pd.read_csv(RAG_DATA_FILENAME,sep=\",\")\n",
    "\n",
    "    # Preprocess all data\n",
    "    docs = preprocess_data(raw_data_df[raw_data_df[RAG_DATA_IMG_COL_NAME].isna()], raw_data_df[ raw_data_df[RAG_DATA_IMG_COL_NAME].notna()])\n",
    "\n",
    "    # Normal Ollama LLM for graph extraction\n",
    "    llm = OllamaLLM(model=MULTIMODAL_INFERENCE_MODEL, temperature=0.0)\n",
    "\n",
    "    transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        #allowed_nodes=[\"Person\", \"Organization\", \"Event\"],\n",
    "        #node_properties=True\n",
    "    )\n",
    "    \n",
    "    # 3. Extract graph\n",
    "    graph_docs = transformer.convert_to_graph_documents(docs)\n",
    "    graph_docs = extracted_graph_post_treatment(graph_docs)\n",
    "\n",
    "    #Empty graph \n",
    "    driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "    with driver.session() as session:\n",
    "\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    driver.close()\n",
    "    \n",
    "    # Store Knowledge Graph in Neo4j\n",
    "    graph_store = Neo4jGraph(url=NEO4J_SERVER_URL, username=NEO4J_LOGIN, password=NEO4J_PWD, database=NEO4J_DB_NAME)\n",
    "    #graph_store.write_graph(graph_docs)\n",
    "\n",
    "    graph_store.add_graph_documents(graph_docs, include_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ffec413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def populate_neo4j_graph_from_benchmark_ds(ds):    \n",
    "    \n",
    "    raw_data_df = ds\n",
    "\n",
    "    # Preprocess all data\n",
    "    docs = preprocess_benchmark_data(raw_data_df[BENCH_DATA_GT_IMG_COL_NAME])\n",
    "\n",
    "    # Normal Ollama LLM for graph extraction\n",
    "    llm = OllamaLLM(model=MULTIMODAL_INFERENCE_MODEL, temperature=0.0)\n",
    "\n",
    "    transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        #allowed_nodes=[\"Person\", \"Organization\", \"Event\"],\n",
    "        #node_properties=True\n",
    "    )\n",
    "    \n",
    "    # 3. Extract graph\n",
    "    graph_docs = transformer.convert_to_graph_documents(docs)\n",
    "    graph_docs = extracted_graph_post_treatment(graph_docs)\n",
    "\n",
    "        \n",
    "    # Store Knowledge Graph in Neo4j\n",
    "    graph_store = Neo4jGraph(url=NEO4J_SERVER_URL, username=NEO4J_LOGIN, password=NEO4J_PWD, database=NEO4J_DB_NAME)\n",
    "\n",
    "\n",
    "    graph_store.add_graph_documents(graph_docs, include_source=True)\n",
    "\n",
    "    graph_store._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df4ddfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770ad7845c184392ab3302c381afadad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bcdac022a84dc284188123e967398b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    ds = load_dataset(\"uclanlp/MRAG-Bench\")\n",
    "    #populate_neo4j_graph()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrdiouf\\AppData\\Local\\Temp\\ipykernel_25520\\1063344752.py:23: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph_store = Neo4jGraph(url=NEO4J_SERVER_URL, username=NEO4J_LOGIN, password=NEO4J_PWD, database=NEO4J_DB_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n",
      "Dataset({\n",
      "    features: ['id', 'aspect', 'scenario', 'image', 'gt_images', 'question', 'A', 'B', 'C', 'D', 'answer_choice', 'answer', 'image_type', 'source', 'retrieved_images'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Record num : 1\n",
      "Num ground truth images : 5\n"
     ]
    }
   ],
   "source": [
    "#Empty graph \n",
    "\"\"\"\n",
    "driver = GraphDatabase.driver(uri=NEO4J_SERVER_URL, database=NEO4J_DB_NAME, auth=(NEO4J_LOGIN,NEO4J_PWD))\n",
    "with driver.session() as session:\n",
    "\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "driver.close()\n",
    "\n",
    "for i in range(BENCH_DATA_BASE_SUBSET_SIZE):\n",
    "    #View top data \n",
    "    subset = ds[\"test\"].select([i])\n",
    "\n",
    "    print(subset)\n",
    "\n",
    "\n",
    "    populate_neo4j_graph_from_benchmark_ds(subset)\n",
    "     \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4925367",
   "metadata": {},
   "source": [
    "## Requête incluant le RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e938ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m rag_2_search_results = []\n\u001b[32m      5\u001b[39m rag_3_search_results = []\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m subset = \u001b[43mds\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m].select(\u001b[38;5;28mrange\u001b[39m(BENCH_DATA_BASE_SUBSET_SIZE))\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m subset:\n\u001b[32m     10\u001b[39m     query_img = record[BENCH_DATA_IMG_COL_NAME]\n",
      "\u001b[31mNameError\u001b[39m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    simple_search_results = []\n",
    "    rag_1_search_results = []\n",
    "    rag_2_search_results = []\n",
    "    rag_3_search_results = []\n",
    "\n",
    "    subset = ds[\"test\"].select(range(BENCH_DATA_BASE_SUBSET_SIZE))\n",
    "\n",
    "    for record in subset:\n",
    "        query_img = record[BENCH_DATA_IMG_COL_NAME]\n",
    "        base_question = record[BENCH_DATA_QUESTION_COL_NAME]\n",
    "        option_a = record[BENCH_DATA_CHOICE_A_COL_NAME]\n",
    "        option_b = record[BENCH_DATA_CHOICE_B_COL_NAME]\n",
    "        option_c = record[BENCH_DATA_CHOICE_C_COL_NAME]\n",
    "        option_d = record[BENCH_DATA_CHOICE_D_COL_NAME]\n",
    "        right_answer = record[BENCH_DATA_ANSWER_TEXT_COL_NAME]\n",
    "\n",
    "        query_question = format_query_question(base_question,option_a,option_b,option_c,option_d)\n",
    "        query_img_path = save_img_to_folder(USER_QUERY_IMAGE_SEARCH_FOLDER,query_img)\n",
    "\n",
    "        simple_search = main_mllm_img_with_query_search(query_img_path,query_question)\n",
    "        rag_1_search, img_rag_1_subgraph = search_main_mllm_from_img_with_query_and_rag_context(query_img_path, query_question, max_graph_depth=1, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)\n",
    "        rag_2_search, img_rag_2_subgraph = search_main_mllm_from_img_with_query_and_rag_context(query_img_path, query_question, max_graph_depth=2, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)\n",
    "        rag_3_search, img_rag_3_subgraph = search_main_mllm_from_img_with_query_and_rag_context(query_img_path, query_question, max_graph_depth=3, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)\n",
    "\n",
    "        print(simple_search)\n",
    "        print(rag_1_search)\n",
    "        print(rag_2_search)\n",
    "        print(rag_3_search)\n",
    "\n",
    "        simple_search_results.append(simple_search == right_answer)\n",
    "        rag_1_search_results.append(rag_1_search == right_answer)\n",
    "        rag_2_search_results.append(rag_2_search == right_answer)\n",
    "        rag_3_search_results.append(rag_3_search == right_answer)\n",
    "    \n",
    "    global_search_results = [simple_search_results,rag_1_search_results,rag_2_search_results,rag_3_search_results]\n",
    "\n",
    "    global_num_good_answers = [answers.sum() for answers in global_search_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec34b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(global_num_good_answers)\n",
    "    plt.xticks([\"Simple\", \"RAG_1\", \"RAG_2\", \"RAG_3\"])\n",
    "    plt.xlabel(\"Type of Search\")\n",
    "    plt.ylabel(\"Number of good results\")\n",
    "    plt.title(\"RAG search performance evaluation\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(img_rag_1_subgraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946290c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(img_rag_2_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(img_rag_3_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    # Query 1\n",
    "    search1 = \"Where does Alice work?\"\n",
    "\n",
    "    # Query 2\n",
    "    search2 = \"Who works for OpenWidgets?\"\n",
    "\n",
    "    search3 = \"black\"\n",
    "    search4 = \"describe happy people\"\n",
    "\n",
    "    search_result, rag_subgraph = search_from_txt_with_rag_context(search4, max_graph_depth=RAG_MAX_GRAPH_DEPTH, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)\n",
    "    file_path = get_user_img_search_path('Man.jpg') # Replace with the actual path to your image\n",
    "    img_search_result, img_rag_subgraph = search_query_mllm_from_img_with_rag_context(file_path,max_graph_depth=RAG_MAX_GRAPH_DEPTH, num_top_results=RAG_QUERY_NUM_TOP_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69001b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba87609",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    print(img_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(rag_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3604882",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    visualize_result_with_yfiles(img_rag_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    visualize_all_graph_with_yfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #search_main_mllm_from_img_with_query_and_rag_context(img_path, user_query, max_graph_depth=1, num_top_results=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIT_DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
